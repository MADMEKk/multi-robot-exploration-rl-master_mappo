# Default configuration for multi-robot exploration with MAPPO

environment:
  map_number: 1
  number_of_robots: 3
  max_steps_per_episode: 500
  goal_radius: 0.5
  
  # Robot velocity constraints
  max_linear_vel: 0.6
  min_linear_vel: 0.05
  max_angular_vel: 0.5
  min_angular_vel: -0.5
  
  # Rewards
  goal_reward: 20.0
  collision_reward: -20.0
  time_penalty: -0.1
  movement_reward: 0.2
  slow_movement_penalty: -0.5
  excessive_rotation_penalty: -0.2
  
  # Exploration settings
  exploration_reward_scale: 0.5
  individual_exploration_scale: 0.2
  max_acceptable_overlap: 0.2
  overlap_penalty_scale: 0.3
  
  # Map dimensions
  map_size: [20.0, 20.0]
  exploration_grid_resolution: 0.5

algorithm:
  # Network architecture
  actor_hidden_dim_1: 512
  actor_hidden_dim_2: 512
  critic_hidden_dim_1: 512
  critic_hidden_dim_2: 512
  
  # PPO hyperparameters
  gamma: 0.99
  gae_lambda: 0.95
  clip_param: 0.2
  value_coef: 0.5
  entropy_coef: 0.01
  
  # Learning rates
  actor_lr: 3.0e-4
  critic_lr: 1.0e-3
  lr_decay_rate: 0.9999
  min_lr: 1.0e-5
  
  # Buffer
  buffer_size: 1000000
  batch_size: 2048
  max_episodes_in_buffer: 50
  
  # Training
  update_epochs: 4
  max_grad_norm: 0.5

experiment:
  name: "default_experiment"
  num_episodes: 10000
  evaluate_interval: 100
  save_interval: 50
  print_interval: 10
  
  # Training metrics
  metrics_dir: "training_metrics"
  plot_interval: 50
  
  # Checkpoint settings
  checkpoint_dir: "deep_learning_weights/mappo"
  save_best_only: false 